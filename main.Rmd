---
title: "main"
author: "Wrzesie≈Ñ Wojciech, Grudkowski Artur"
date: "22 05 2021"
output: html_document
---

```{r setup, include=FALSE}
JOB_CORES = parallel::detectCores()

source('./src/project_setup.R')
```

```{r load_src_files}
source('./src/load_data.R')
source('./src/transform_types.R')
source('./src/show_base_stats.R')
source('./src/remove_missing_vals.R')
source('./src/add_dummy_vars.R')
source('./src/create_model_formula.R')
source('./src/divide_into_folds.R')
source('./src/model_fit.R')
source('./src/model_predict.R')
source('./src/model_evaluate.R')
source('./src/cross_validate.R')
source('./src/preprocess.R')
source('./src/destruct_formula.R')
source('./src/normalize_data.R')
source('./src/utils/mode.R')
source('./src/base_imputation.R')
```


# Load data
```{r}
data <- load_data()

train_base <- data$train
test_base <- data$test
# train_base %>% head(5) %>% knitr::kable()
# test <- data$test
```


# Print base stats
```{r}
print(train_base)
print(test_base)
show_base_stats(train_base)
```

# Normalize data
```{r}
normalization_model <- create_normalize_data_model(train_base, c("training_hours"))

train_normalized <- train_base %>% normalize_data(normalization_model)
test_normalized <- test_base %>% normalize_data(normalization_model)
```

# Transform data
```{r}
categorical_vars_names <- c('city', 
                            'gender', 
                            'relevent_experience', 
                            'enrolled_university',
                            'education_level',
                            'major_discipline',
                            'experience',
                            'company_size',
                            'company_type',
                            'last_new_job')

#train_transformed <- train_normalized %>% transform_types(categorical_vars_names) %>% remove_missing_vals
#test_transformed <- test_normalized %>% transform_types(categorical_vars_names) %>% remove_missing_vals
train_transformed <- train_normalized %>% transform_types(categorical_vars_names)
test_transformed <- test_normalized %>% transform_types(categorical_vars_names)

# Remove useless columns
train_transformed <- train_transformed %>% dplyr::select(!c("enrollee_id"))
test_transformed <- test_transformed %>% dplyr::select(!c("enrollee_id"))
```

# Create folds for cross validation
```{r}
train_folded <- train_transformed %>% divide_into_folds
print(train_folded)
``` 

# Train SVM model and predict
```{r}
independent_var_vec <- names(train_transformed)
# Remove 'target' from vector
independent_var_vec <- independent_var_vec[! independent_var_vec %in% c('target')]

model_formula <- create_model_formula('target', independent_var_vec)

cv_result <- cross_validate_fn(
  data = train_folded,
  formulas = c(model_formula),
  model_fn = svm_model_fn,
  predict_fn = model_predict,
  preprocess_fn = preprocess_fn,
  hyperparameters = list(
    "kernel" = c("linear", "radial"),
    "cost" = c(1, 5, 10)
  ),
  fold_cols = c(".folds"),
  type = "binomial",
  parallel = TRUE,
  verbose = TRUE
)

cv_result

```
## Results
### Balanced Accuracy, F1, MCC, Model ID, AUC
```{r}
cv_result %>% 
  dplyr::mutate(`Model ID` = 1:nrow(cv_result)) %>% 
  dplyr::arrange(dplyr::desc(AUC)) %>% 
  select_definitions(additional_includes = c("Balanced Accuracy", "F1", "MCC", "Model ID", "AUC")) %>%
  dplyr::select(-c('Fixed', 'Dependent')) %>% 
  kable(digits = 5)
```

### Best result
```{r}
best_model_id <- 2
best_result <- cv_result %>% dplyr::slice(best_model_id)

best_result
print(best_result$HParams[[1]])
print(best_result$Fixed)
print(best_result$Dependent)
```

### Confusion matrix
```{r}
plot_confusion_matrix(cv_result$`Confusion Matrix`[[best_model_id]], add_sums = TRUE)
```

### ROC
```{r}
plot(best_result$ROC[[1]]$.folds)
```

### Train the best model on the whole training dataset
```{r}
best_model <- svm_model_fn(
  train_data = train_transformed,
  formula = as.formula(paste(c(best_result$Dependent, best_result$Fixed), collapse = ' ~ ')),
  hyperparameters = best_result$HParams[[1]]
)
```

### Predict on test dataset
```{r}
predict <- model_predict(
  test_data = test_transformed, 
  model = best_model
)

test_transformed[["predicted_class"]] <- predict

eval <- model_evaluate(
  test_set = test_transformed, 
  target_col = "target", 
  prediction_cols = "predicted_class"
)
```

### Plot confusion matrix
```{r}
plot_confusion_matrix(eval, add_sums = TRUE)
```
### ROC
```{r}
plot_confusion_matrix(eval, add_sums = TRUE)
```

# Train XGBoost model and predict
```{r}
model_formula <- create_model_formula('target', '.')
dummy_model <- dummyVars('~ .', data=train_transformed, fullRank=TRUE)

cv_result <- cross_validate_fn(
  data = train_folded,
  formulas = c(model_formula),
  model_fn = xgboost_model_fn,
  predict_fn = xgboost_predict_fn,
  preprocess_fn = preprocess_fn,
  hyperparameters = list(
    "nthread" = c(JOB_CORES),
    "nround" = c(1, 4, 10, 100),
    "max_depth" = c(1, 5, 10, 50, 100),
    "dummy_model" = list(dummy_model)
  ),
  fold_cols = c(".folds"),
  type = "binomial",
  parallel = FALSE,
  verbose = TRUE
)

cv_result

```
## Results
### Balanced Accuracy, F1, MCC, Model ID, AUC
```{r}
cv_result %>% 
  dplyr::mutate(`Model ID` = 1:nrow(cv_result)) %>% 
  dplyr::arrange(dplyr::desc(AUC)) %>% 
  select_definitions(additional_includes = c("Balanced Accuracy", "F1", "MCC", "Model ID", "AUC")) %>%
  dplyr::select(-c('Fixed', 'Dependent', 'dummy_model')) %>% 
  kable(digits = 5)
```

### Best result
```{r}
best_model_id <- 7
best_result <- cv_result %>% dplyr::slice(best_model_id)

best_result
print(best_result$HParams[[1]])
print(best_result$Fixed)
print(best_result$Dependent)
print(best_result$Preprocess)
```

### Confusion matrix
```{r}
plot_confusion_matrix(cv_result$`Confusion Matrix`[[best_model_id]], add_sums = TRUE)
```


### ROC
```{r}
plot(best_result$ROC[[1]]$.folds)
```

### Train the best model on the whole training dataset
```{r}
best_model_formula <- as.formula(paste(c(best_result$Dependent, best_result$Fixed), collapse = ' ~ '))

preprocessed_data <- preprocess_fn(train_transformed, test_transformed, best_result$HParams[[1]])

best_model <- xgboost_model_fn(
  train_data = preprocessed_data$train,
  formula = best_model_formula,
  hyperparameters = best_result$HParams[[1]]
)
```

### Predict on test dataset
```{r}
predict <- xgboost_predict_fn(
  formula = best_model_formula,
  test_data = preprocessed_data$test, 
  model = best_model,
  hyperparameters = best_result$HParams[[1]]
)

preprocessed_data$test[["predicted_class"]] <- predict


eval <- model_evaluate(
  test_set = preprocessed_data$test, 
  target_col = "target", 
  prediction_cols = "predicted_class"
)
eval
```

### Plot confusion matrix
```{r}
plot_confusion_matrix(eval, add_sums = TRUE)
```
### ROC
```{r}
plot(eval$ROC[[1]])
```

## LOGISTIC REGRESSION
```{r}
# Temporary, later move to separate scripts 

lg_model_fn <- function(train_data, formula, hyperparameters){
  stats::glm(
    formula = formula,
    data = train_data,
    family = hyperparameters[["family"]]
  )
}

lg_predict_fn <- function(test_data, model, formula, hyperparameters, train_data){
  stats::predict(
    object = model,
    newdata = test_data,
    type = "response")
}

train_folded <- train_dummy %>% divide_into_folds

lg_result <- cross_validate_fn(
  data = train_folded,
  formulas = c("target ~ ."),
  model_fn = lg_model_fn,
  predict_fn = lg_predict_fn,
  hyperparameters = list(
    "family" = c("binomial")
  ),
  fold_cols = c(".folds"),
  type = "binomial",
  parallel = TRUE,
  verbose = TRUE
)
lg_result

best_result <- lg_result %>% 
  head(1)

lg_result %>% 
  dplyr::mutate(`Model ID` = 1:nrow(lg_result)) %>% 
  dplyr::arrange(dplyr::desc(AUC)) %>% 
  select_definitions(additional_includes = c("Balanced Accuracy", "F1", "MCC", "Model ID", "AUC")) %>%
  dplyr::select(-c('Fixed', 'Dependent')) %>% 
  kable(digits = 5)

plot_confusion_matrix(lg_result$`Confusion Matrix`[[1]], add_sums = TRUE)

plot(best_result$ROC[[1]]$.folds)
```

## RANDOM FOREST
```{r}
# Temporary, later move to separate scripts 

independent_var_vec <- names(train_transformed)
# Remove 'target' from vector
independent_var_vec <- independent_var_vec[! independent_var_vec %in% c('target')]

model_formula <- create_model_formula('target', independent_var_vec)

tree_model_fn <- function(train_data, formula, hyperparameters){
  randomForest(
    formula = formula,
    data = train_data,
    ntree = hyperparameters[["ntree"]]
  )
}

tree_predict_fn <- function(test_data, model, formula, hyperparameters, train_data){
  stats::predict(
    object = model,
    newdata = test_data,
    type = "response")
}

train_folded <- train_transformed %>% divide_into_folds

tree_result <- cross_validate_fn(
  data = train_folded,
  formulas = c("target ~ ."),
  model_fn = tree_model_fn,
  predict_fn = tree_predict_fn,
  hyperparameters = list(
    "ntree" = c(250)
  ),
  fold_cols = c(".folds"),
  type = "binomial",
  parallel = TRUE,
  verbose = TRUE
)

tree_result

best_result <- tree_result %>% 
  head(1)

tree_result %>% 
  dplyr::mutate(`Model ID` = 1:nrow(tree_result)) %>% 
  dplyr::arrange(dplyr::desc(AUC)) %>% 
  select_definitions(additional_includes = c("Balanced Accuracy", "F1", "MCC", "Model ID", "AUC")) %>%
  dplyr::select(-c('Fixed', 'Dependent')) %>% 
  kable(digits = 5)

plot_confusion_matrix(tree_result$`Confusion Matrix`[[1]], add_sums = TRUE)

plot(best_result$ROC[[1]]$.folds)

```


```{r}

```