---
title: "main"
author: "Wrzesie≈Ñ Wojciech, Grudkowski Artur"
date: "22 05 2021"
output: html_document
---

```{r setup, include=FALSE}
JOB_CORES = parallel::detectCores()

source('./src/project_setup.R')
```

```{r load_src_files}
source('./src/load_data.R')
source('./src/transform_types.R')
source('./src/show_base_stats.R')
source('./src/remove_missing_vals.R')
source('./src/add_dummy_vars.R')
source('./src/create_model_formula.R')
source('./src/divide_into_folds.R')
source('./src/model_fit.R')
source('./src/model_predict.R')
source('./src/model_evaluate.R')
source('./src/cross_validate.R')
source('./src/preprocess.R')
source('./src/destruct_formula.R')
```


# Load data
```{r}
data <- load_data()

train_base <- data$train
test_base <- data$test
# train_base %>% head(5) %>% knitr::kable()
# test <- data$test
```


# Print base stats
```{r}
print(train_base)
print(test_base)
show_base_stats(train_base)
```

# Transform data
```{r}
  categorical_vars_names <- c('city', 
                              'gender', 
                              'relevent_experience', 
                              'enrolled_university',
                              'education_level',
                              'major_discipline',
                              'experience',
                              'company_size',
                              'company_type',
                              'last_new_job')

train_transformed <- train_base %>% transform_types(categorical_vars_names) %>% remove_missing_vals
test_transformed <- test_base %>% transform_types(categorical_vars_names) %>% remove_missing_vals

# Remove useless columns
train_transformed <- train_transformed %>% dplyr::select(!c("enrollee_id", "city"))
test_transformed <- test_transformed %>% dplyr::select(!c("enrollee_id", "city"))

summary(train_transformed)
skim(train_transformed)
print(train_transformed)
```
# Add dummy vars
```{r}
# model_vars <- c('gender', 
#                 'relevent_experience', 
#                 'education_level',
#                 'major_discipline',
#                 'experience',
#                 'company_type',
#                 'last_new_job')
# # Dropp city !!!!
# model_formula <- create_model_formula('target', model_vars)
# model_formula < as.formula('~ .')
dummy_model <- dummyVars('~ .', data=train_transformed, fullRank=TRUE)

train_dummy <- train_transformed %>% add_dummy_vars(dummy_model)
test_dummy <- test_transformed %>% add_dummy_vars(dummy_model)
print(train_dummy)
```

# Create folds for cross validation
```{r}
# Skip dummy vars for now
# train_folded <- train_dummy %>% divide_into_folds

train_folded <- train_dummy %>% divide_into_folds
print(train_folded)
``` 

# Train SVM model and predict
```{r}
independent_var_vec <- names(train_transformed)
# Remove 'target' from vector
independent_var_vec <- independent_var_vec[! independent_var_vec %in% c('target')]

model_formula <- create_model_formula('target', independent_var_vec)

train_folded <- train_transformed %>% divide_into_folds

cv_result <- cross_validate_fn(
  data = train_folded,
  formulas = c(model_formula),
  model_fn = svm_model_fn,
  predict_fn = model_predict,
  preprocess_fn = preprocess_fn,
  hyperparameters = list(
    "kernel" = c("linear", "radial"),
    "cost" = c(10, 5, 10)
  ),
  fold_cols = c(".folds"),
  type = "binomial",
  parallel = TRUE,
  verbose = TRUE
)

cv_result

```
## Results
### Balanced Accuracy, F1, MCC, Model ID, AUC
```{r}
cv_result %>% 
  dplyr::mutate(`Model ID` = 1:nrow(cv_result)) %>% 
  dplyr::arrange(dplyr::desc(AUC)) %>% 
  select_definitions(additional_includes = c("Balanced Accuracy", "F1", "MCC", "Model ID", "AUC")) %>%
  dplyr::select(-c('Fixed', 'Dependent')) %>% 
  kable(digits = 5)
```

### Best result
```{r}
best_model_id <- 1
best_result <- cv_result %>% dplyr::slice(best_model_id)

best_result
print(best_result$HParams[[1]])
print(best_result$Fixed)
print(best_result$Dependent)
```

### Confusion matrix
```{r}
plot_confusion_matrix(cv_result$`Confusion Matrix`[[best_model_id]], add_sums = TRUE)
```

### ROC
```{r}
plot(best_result$ROC[[1]]$.folds)
```

### Train the best model on the whole training dataset
```{r}
best_model <- svm_model_fn(
  train_data = train_transformed,
  formula = as.formula(paste(c(best_result$Dependent, best_result$Fixed), collapse = ' ~ ')),
  hyperparameters = best_result$HParams[[1]]
)
```

### Predict on test dataset
```{r}
predict <- model_predict(
  test_data = test_transformed[-12], 
  model = best_model
)

test_transformed[["predicted_class"]] <- predict

eval <- model_evaluate(
  test_set = test_transformed, 
  target_col = "target", 
  prediction_cols = "predicted_class"
)
eval
```

### Plot confusion matrix
```{r}
plot_confusion_matrix(eval, add_sums = TRUE)
```
### ROC
```{r}
plot(best_result$ROC[[1]]$.folds)
```

# Train XGBoost model and predict
```{r}
independent_var_vec <- names(train_dummy)
# Remove 'target' from vector
independent_var_vec <- independent_var_vec[! independent_var_vec %in% c('target')]

model_formula <- create_model_formula('target', independent_var_vec)

train_folded <- train_dummy %>% divide_into_folds

cv_result <- cross_validate_fn(
  data = train_folded,
  formulas = c(model_formula),
  model_fn = xgboost_model_fn,
  predict_fn = xgboost_predict_fn,
  hyperparameters = list(
    "nthread" = c(JOB_CORES),
    "nround" = c(1, 4, 10, 100),
    "max_depth" = c(1, 5, 10, 50, 100)
  ),
  fold_cols = c(".folds"),
  type = "binomial",
  parallel = FALSE,
  verbose = TRUE
)

cv_result

```
## Results
### Balanced Accuracy, F1, MCC, Model ID, AUC
```{r}
cv_result %>% 
  dplyr::mutate(`Model ID` = 1:nrow(cv_result)) %>% 
  dplyr::arrange(dplyr::desc(AUC)) %>% 
  select_definitions(additional_includes = c("Balanced Accuracy", "F1", "MCC", "Model ID", "AUC")) %>%
  dplyr::select(-c('Fixed', 'Dependent')) %>% 
  kable(digits = 5)
```

### Best result
```{r}
best_model_id <- 1
best_result <- cv_result %>% dplyr::slice(best_model_id)

best_result
print(best_result$HParams[[1]])
print(best_result$Fixed)
print(best_result$Dependent)
```

### Confusion matrix
```{r}
plot_confusion_matrix(cv_result$`Confusion Matrix`[[best_model_id]], add_sums = TRUE)
```

### ROC
```{r}
plot(best_result$ROC[[1]]$.folds)
```

### Train the best model on the whole training dataset
```{r}
best_model_formula <- as.formula(paste(c(best_result$Dependent, best_result$Fixed), collapse = ' ~ '))

best_model <- xgboost_model_fn(
  train_data = train_dummy,
  formula = best_model_formula,
  hyperparameters = best_result$HParams[[1]]
)
```

### Predict on test dataset
```{r}
predict <- xgboost_predict_fn(
  formula = best_model_formula,
  test_data = test_dummy, 
  model = best_model
)

test_dummy[["predicted_class"]] <- predict

eval <- model_evaluate(
  test_set = test_dummy, 
  target_col = "target", 
  prediction_cols = "predicted_class"
)
eval
```

### Plot confusion matrix
```{r}
plot_confusion_matrix(eval, add_sums = TRUE)
```
### ROC
```{r}
plot(eval$ROC[[1]])
```

# Train logistic regression model and predict
```{r}
independent_var_vec <- names(train_dummy)
# Remove 'target' from vector
independent_var_vec <- independent_var_vec[! independent_var_vec %in% c('target')]

model_formula <- create_model_formula('target', independent_var_vec)

train_folded <- train_dummy %>% divide_into_folds

lg_result <- cross_validate_fn(
  data = train_folded,
  formulas = model_formula,
  model_fn = lg_model_fn,
  predict_fn = lg_predict_fn,
  hyperparameters = list(
    "family" = c("binomial")
  ),
  fold_cols = c(".folds"),
  type = "binomial",
  parallel = TRUE,
  verbose = TRUE
)

lg_result
```
## Results
### Balanced Accuracy, F1, MCC, Model ID, AUC
```{r}
lg_result %>% 
  dplyr::mutate(`Model ID` = 1:nrow(lg_result)) %>% 
  dplyr::arrange(dplyr::desc(AUC)) %>% 
  select_definitions(additional_includes = c("Balanced Accuracy", "F1", "MCC", "Model ID", "AUC")) %>%
  dplyr::select(-c('Fixed', 'Dependent')) %>% 
  kable(digits = 5)
```
### Best result
```{r}
best_model_id <- 1
best_result <- lg_result %>% dplyr::slice(best_model_id)

print(best_result)
print(best_result$HParams[[1]])
print(best_result$Fixed)
print(best_result$Dependent)
```

### Confusion matrix
```{r}
plot_confusion_matrix(lg_result$`Confusion Matrix`[[best_model_id]], add_sums = TRUE)
```

### ROC
```{r}
plot(best_result$ROC[[1]]$.folds)
```

### Train the best model on the whole training dataset
```{r}
best_model_formula <- as.formula(paste(c(best_result$Dependent, best_result$Fixed), collapse = ' ~ '))

best_model <- lg_model_fn(
  train_data = train_dummy,
  formula = best_model_formula,
  hyperparameters = best_result$HParams[[1]]
)
```

### Predict on test dataset
```{r}
predict <- lg_predict_fn(
  formula = best_model_formula,
  test_data = test_dummy, 
  model = best_model
)

test_dummy[["predicted_class"]] <- predict

eval <- model_evaluate(
  test_set = test_dummy, 
  target_col = "target", 
  prediction_cols = "predicted_class"
)
eval
```

### Plot confusion matrix
```{r}
plot_confusion_matrix(eval, add_sums = TRUE)
```
### ROC
```{r}
plot(eval$ROC[[1]])
```

# Train Random Forest model and predict
```{r}
independent_var_vec <- names(train_transformed)
# Remove 'target' from vector
independent_var_vec <- independent_var_vec[! independent_var_vec %in% c('target')]

model_formula <- create_model_formula('target', independent_var_vec)

train_folded <- train_transformed %>% divide_into_folds

# parameters to test: ntree: (100, 250, 500, 1000)

forest_result <- cross_validate_fn(
  data = train_folded,
  formulas = model_formula,
  model_fn = forest_model_fn,
  predict_fn = forest_predict_fn,
  hyperparameters = list(
    "ntree" = 2
  ),
  fold_cols = c(".folds"),
  type = "binomial",
  parallel = TRUE,
  verbose = TRUE
)

forest_result

```
## Results
### Balanced Accuracy, F1, MCC, Model ID, AUC
```{r}
forest_result %>% 
  dplyr::mutate(`Model ID` = 1:nrow(lg_result)) %>% 
  dplyr::arrange(dplyr::desc(AUC)) %>% 
  select_definitions(additional_includes = c("Balanced Accuracy", "F1", "MCC", "Model ID", "AUC")) %>%
  dplyr::select(-c('Fixed', 'Dependent')) %>% 
  kable(digits = 5)
```
### Best result
```{r}
best_model_id <- 1
best_result <- forest_result %>% dplyr::slice(best_model_id)

print(best_result)
print(best_result$HParams[[1]])
print(best_result$Fixed)
print(best_result$Dependent)
```

### Confusion matrix
```{r}
plot_confusion_matrix(forest_result$`Confusion Matrix`[[best_model_id]], add_sums = TRUE)
```

### ROC
```{r}
plot(best_result$ROC[[1]]$.folds)
```

### Train the best model on the whole training dataset
```{r}
best_model_formula <- as.formula(paste(c(best_result$Dependent, best_result$Fixed), collapse = ' ~ '))

best_model <- forest_model_fn(
  train_data = train_transformed,
  formula = best_model_formula,
  hyperparameters = best_result$HParams[[1]]
)
```

### Predict on test dataset
```{r}
predict <- forest_predict_fn(
  formula = best_model_formula,
  test_data = test_transformed, 
  model = best_model
)

test_dummy[["predicted_class"]] <- predict
predict

eval <- model_evaluate(
  test_set = test_transformed, 
  target_col = "target", 
  prediction_cols = "predicted_class"
)
eval
```

### Plot confusion matrix
```{r}
plot_confusion_matrix(eval, add_sums = TRUE)
```
### ROC
```{r}
plot(eval$ROC[[1]])
```
