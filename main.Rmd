---
title: "main"
author: "Wrzesie≈Ñ Wojciech, Grudkowski Artur"
date: "22 05 2021"
output: html_document
---

```{r setup, include=FALSE}
#JOB_CORES = parallel::detectCores()
JOB_CORES = 12
source('./src/project_setup.R')
```

```{r load_src_files}
source('./src/load_data.R')
source('./src/transform_types.R')
source('./src/show_base_stats.R')
source('./src/remove_missing_vals.R')
source('./src/add_dummy_vars.R')
source('./src/create_model_formula.R')
source('./src/divide_into_folds.R')
source('./src/model_fit.R')
source('./src/model_predict.R')
source('./src/model_evaluate.R')
source('./src/preprocess.R')
source('./src/destruct_formula.R')
source('./src/normalize_data.R')
source('./src/utils/mode.R')
source('./src/utils/calc_weights.R')
source('./src/base_imputation.R')
```


# Load data
```{r}
data <- load_data()

train_base <- data$train
test_base <- data$test
# train_base %>% head(5) %>% knitr::kable()
# test <- data$test
```


# Print base stats
```{r}
print(train_base)
print(test_base)
show_base_stats(train_base)
```

# Normalize data
```{r}
normalization_model <- create_normalize_data_model(train_base, c("training_hours"))

train_normalized <- train_base %>% normalize_data(normalization_model)
test_normalized <- test_base %>% normalize_data(normalization_model)
```

# Transform data
```{r}
# List of categorical with their unque values
categorical_vars_names <- list(city = unique(union(train_base$city, test_base$city)), 
                            gender = unique(union(train_base$gender, test_base$gender)), 
                            relevent_experience = unique(union(train_base$relevent_experience, test_base$relevent_experience)), 
                            enrolled_university = unique(union(train_base$enrolled_university, test_base$enrolled_university)),
                            education_level = unique(union(train_base$education_level, test_base$education_level)),
                            major_discipline = unique(union(train_base$major_discipline, test_base$major_discipline)),
                            experience = unique(union(train_base$experience, test_base$experience)),
                            company_size = unique(union(train_base$company_size, test_base$company_size)),
                            company_type = unique(union(train_base$company_type, test_base$company_type)),
                            last_new_job = unique(union(train_base$last_new_job, test_base$last_new_job)))

#train_transformed <- train_normalized %>% transform_types(categorical_vars_names) %>% remove_missing_vals
#test_transformed <- test_normalized %>% transform_types(categorical_vars_names) %>% remove_missing_vals
train_transformed <- train_normalized %>% transform_types(categorical_vars_names)
test_transformed <- test_normalized %>% transform_types(categorical_vars_names)

# Remove useless columns
train_transformed <- train_transformed %>% dplyr::select(!c("enrollee_id"))
test_transformed <- test_transformed %>% dplyr::select(!c("enrollee_id"))
```
# Create folds for cross validation
```{r}
train_folded <- train_transformed %>% divide_into_folds
print(train_folded)
``` 

# Train SVM model and predict
```{r}
model_formula <- create_model_formula('target', '.')

cv_result <- cross_validate_fn(
  data = train_folded,
  formulas = c(model_formula),
  model_fn = svm_model_fn,
  predict_fn = model_predict,
  preprocess_fn = preprocess_fn,
  hyperparameters = list(
    "kernel" = c("linear", "radial"),
    "cost" = c(1, 2.5, 5),
    "use_rose" = c(TRUE)
  ),
  fold_cols = c(".folds"),
  type = "binomial",
  parallel = TRUE,
  verbose = TRUE
)

cv_result

```
## Results
### Balanced Accuracy, F1, MCC, Model ID, AUC
```{r}
cv_result %>% 
  dplyr::mutate(`Model ID` = 1:nrow(cv_result)) %>% 
  dplyr::arrange(dplyr::desc(AUC)) %>% 
  select_definitions(additional_includes = c("Balanced Accuracy", "F1", "MCC", "Model ID", "AUC")) %>%
  dplyr::select(-c('Fixed', 'Dependent')) %>% 
  kable(digits = 5)
```

### Best result
```{r}
saveRDS(cv_result, paste(c("results/svm/cv_result_svm_boruta", as.character(format(Sys.time(), "%Y-%m-%d_%H:%M:%S"))), collapse = '_'))

best_model_id <- 2
best_result <- cv_result %>% dplyr::slice(best_model_id)

best_result
print(best_result$HParams[[1]])
print(best_result$Fixed)
print(best_result$Dependent)
```

### Confusion matrix
```{r}
plot_confusion_matrix(cv_result$`Confusion Matrix`[[best_model_id]], add_sums = TRUE)
```

### ROC
```{r}
plot(best_result$ROC[[1]]$.folds)
```

### Train the best model on the whole training dataset
```{r}
best_model_formula <- as.formula(paste(c(best_result$Dependent, best_result$Fixed), collapse = ' ~ '))

preprocessed_data <- preprocess_fn(train_transformed, test_transformed, best_model_formula, best_result$HParams[[1]])

best_model <- svm_model_fn(
  train_data = preprocessed_data$train,
  formula = best_model_formula,
  hyperparameters = best_result$HParams[[1]]
)

saveRDS(best_model, paste(c("results/svm/best_svm_boruta", as.character(format(Sys.time(), "%Y-%m-%d_%H:%M:%S"))), collapse = '_'))
```

### Predict on test dataset
```{r}
predict <- model_predict(
  formula = best_model_formula,
  test_data = preprocessed_data$test,
  train_data = preprocessed_data$train,
  model = best_model,
  hyperparameters = best_result$HParams[[1]]
)

preprocessed_data$test[["predicted_class"]] <- predict

eval <- model_evaluate(
  test_set = preprocessed_data$test, 
  target_col = "target", 
  prediction_cols = "predicted_class"
)
eval
```

### Plot confusion matrix
```{r}
plot_confusion_matrix(eval, add_sums = TRUE)
```
### ROC
```{r}
plot(eval$ROC[[1]])
```

# Train XGBoost model and predict
```{r}
model_formula <- create_model_formula('target', '.')
dummy_model <- dummyVars('~ .', data=train_transformed, fullRank=TRUE)

cv_result <- cross_validate_fn(
  data = train_folded,
  formulas = c(model_formula),
  model_fn = xgboost_model_fn,
  predict_fn = xgboost_predict_fn,
  preprocess_fn = preprocess_fn,
  hyperparameters = list(
    "nthread" = c(JOB_CORES),
    "nround" = c(4),
    "max_depth" = c( 5),
    "dummy_model" = list(dummy_model),
    "use_rose" = c(TRUE, FALSE),
    "use_imputation" = c(TRUE, FALSE)
  ),
  fold_cols = c(".folds"),
  type = "binomial",
  parallel = FALSE,
  verbose = TRUE
)

cv_result

```
## Results
### Balanced Accuracy, F1, MCC, Model ID, AUC
```{r}
cv_result %>% 
  dplyr::mutate(`Model ID` = 1:nrow(cv_result)) %>% 
  dplyr::arrange(dplyr::desc(AUC)) %>% 
  select_definitions(additional_includes = c("Balanced Accuracy", "F1", "MCC", "Model ID", "AUC")) %>%
  dplyr::select(-c('Fixed', 'Dependent', 'dummy_model')) %>% 
  kable(digits = 5)
```

### Best result
```{r}
best_model_id <- 4
best_result <- cv_result %>% dplyr::slice(best_model_id)

best_result
print(best_result$HParams[[1]])
print(best_result$Fixed)
print(best_result$Dependent)
print(best_result$Preprocess)
```

### Confusion matrix
```{r}
plot_confusion_matrix(cv_result$`Confusion Matrix`[[best_model_id]], add_sums = TRUE)
```

### ROC
```{r}
plot(best_result$ROC[[1]]$.folds)
```

### Train the best model on the whole training dataset
```{r}
best_model_formula <- as.formula(paste(c(best_result$Dependent, best_result$Fixed), collapse = ' ~ '))

preprocessed_data <- preprocess_fn(train_transformed, test_transformed, best_model_formula, best_result$HParams[[1]])

best_model <- xgboost_model_fn(
  train_data = preprocessed_data$train,
  formula = best_model_formula,
  hyperparameters = best_result$HParams[[1]]
)
```

### Predict on test dataset
```{r}
predict <- xgboost_predict_fn(
  formula = best_model_formula,
  test_data = preprocessed_data$test, 
  model = best_model,
  hyperparameters = best_result$HParams[[1]]
)

preprocessed_data$test[["predicted_class"]] <- predict


eval <- model_evaluate(
  test_set = preprocessed_data$test, 
  target_col = "target", 
  prediction_cols = "predicted_class"
)
eval
```

### Plot confusion matrix
```{r}
plot_confusion_matrix(eval, add_sums = TRUE)
```
### ROC
```{r}
plot(eval$ROC[[1]])
```

# Train logistic regression model and predict
```{r}
model_formula <- create_model_formula('target', '.')
dummy_model <- dummyVars('~ .', data=train_transformed, fullRank=TRUE)

lg_result <- cross_validate_fn(
  data = train_folded,
  formulas = model_formula,
  model_fn = lg_model_fn,
  predict_fn = lg_predict_fn,
  preprocess_fn = preprocess_fn,
  hyperparameters = list(
    "family" = c("binomial"),
    "dummy_model" = list(dummy_model),
    "use_weights" = TRUE
  ),
  fold_cols = c(".folds"),
  type = "binomial",
  parallel = FALSE,
  verbose = TRUE
)

lg_result
```
## Results
### Balanced Accuracy, F1, MCC, Model ID, AUC
```{r}
lg_result %>% 
  dplyr::mutate(`Model ID` = 1:nrow(lg_result)) %>% 
  dplyr::arrange(dplyr::desc(AUC)) %>% 
  select_definitions(additional_includes = c("Balanced Accuracy", "F1", "MCC", "Model ID", "AUC")) %>%
  dplyr::select(-c('Fixed', 'Dependent')) %>% 
  kable(digits = 5)
```
### Best result
```{r}
best_model_id <- 1
best_result <- lg_result %>% dplyr::slice(best_model_id)

print(best_result)
print(best_result$HParams[[1]])
print(best_result$Fixed)
print(best_result$Dependent)
```

### Confusion matrix
```{r}
plot_confusion_matrix(lg_result$`Confusion Matrix`[[best_model_id]], add_sums = TRUE)
```

### ROC
```{r}
plot(best_result$ROC[[1]]$.folds)
```

### Train the best model on the whole training dataset
```{r}
best_model_formula <- as.formula(paste(c(best_result$Dependent, best_result$Fixed), collapse = ' ~ '))

preprocessed_data <- preprocess_fn(train_transformed, test_transformed, best_model_formula, best_result$HParams[[1]])

best_model <- lg_model_fn(
  train_data = preprocessed_data$train,
  formula = best_model_formula,
  hyperparameters = best_result$HParams[[1]]
)
```

### Predict on test dataset
```{r}
predict <- lg_predict_fn(
  formula = best_model_formula,
  test_data = preprocessed_data$test,
  train_data = preprocessed_data$train,
  model = best_model,
  hyperparameters = best_result$HParams[[1]]
)

preprocessed_data$test[["predicted_class"]] <- predict

eval <- model_evaluate(
  test_set = preprocessed_data$test, 
  target_col = "target", 
  prediction_cols = "predicted_class"
)
eval
```

### Plot confusion matrix
```{r}
plot_confusion_matrix(eval, add_sums = TRUE)
```
### ROC
```{r}
plot(eval$ROC[[1]])
```

# Train Random Forest model and predict
```{r}
model_formula <- create_model_formula('target', '.')
# dummy vars are needed for city feature (random forest handles up to 53 categories in categorical variables)
dummy_model <- dummyVars('~ .', data=train_transformed, fullRank=TRUE)

# parameters to test: ntree: (100, 250, 500, 1000)

forest_result <- cross_validate_fn(
  data = train_folded,
  formulas = model_formula,
  model_fn = forest_model_fn,
  predict_fn = forest_predict_fn,
  preprocess_fn = preprocess_fn,
  hyperparameters = list(
    "ntree" = c(200),
    "mtree" = c('DEFAULT'),
    "nodesize" = c(10),
    "dummy_model" = list(dummy_model),
    "use_rose" = c(TRUE, FALSE)
  ),
  fold_cols = c(".folds"),
  type = "binomial",
  parallel = TRUE,
  verbose = TRUE
)

forest_result

```
## Results
### Balanced Accuracy, F1, MCC, Model ID, AUC
```{r}
forest_result %>% 
  dplyr::mutate(`Model ID` = 1:nrow(forest_result)) %>% 
  dplyr::arrange(dplyr::desc(AUC)) %>% 
  select_definitions(additional_includes = c("Balanced Accuracy", "F1", "MCC", "Model ID", "AUC")) %>%
  dplyr::select(-c('Fixed', 'Dependent', 'dummy_model')) %>% 
  kable(digits = 5)
```
### Best result
```{r}
saveRDS(forest_result, paste(c("results/random_forest/cv_result_rf_200_DEF_10_rose", as.character(format(Sys.time(), "%Y-%m-%d_%H:%M:%S"))), collapse = '_'))

best_model_id <- 1
best_result <- forest_result %>% dplyr::slice(best_model_id)

print(best_result)
print(best_result$HParams[[1]])
print(best_result$Fixed)
print(best_result$Dependent)
```

### Confusion matrix
```{r}
plot_confusion_matrix(forest_result$`Confusion Matrix`[[best_model_id]], add_sums = TRUE)
```

### ROC
```{r}
plot(best_result$ROC[[1]]$.folds)
```

### Train the best model on the whole training dataset
```{r}
best_model_formula <- as.formula(paste(c(best_result$Dependent, best_result$Fixed), collapse = ' ~ '))

preprocessed_data <- preprocess_fn(train_transformed, test_transformed, best_model_formula, best_result$HParams[[1]])

best_model <- forest_model_fn(
  train_data = preprocessed_data$train,
  formula = best_model_formula,
  hyperparameters = best_result$HParams[[1]]
)

saveRDS(best_model, paste(c("results/random_forest/best_cv_rf", as.character(format(Sys.time(), "%Y-%m-%d_%H:%M:%S"))), collapse = '_'))
```

### Predict on test dataset
```{r}
predict <- forest_predict_fn(
  formula = best_model_formula,
  test_data = preprocessed_data$test,
  train_data = preprocessed_data$train,
  model = best_model,
  hyperparameters = best_result$HParams[[1]]
)

# randomForest sometimes predict values below 0 (we are using regresion for predicting probabilities), these are values very very close to 0 eg -4.23424e-17
# and because of that abs func is applied
preprocessed_data$test[["predicted_class"]] <- abs(predict)
min(predict)

eval <- model_evaluate(
  test_set = preprocessed_data$test, 
  target_col = "target", 
  prediction_cols = "predicted_class"
)
eval
```

### Plot confusion matrix
```{r}
plot_confusion_matrix(eval, add_sums = TRUE)
```
### ROC
```{r}
plot(eval$ROC[[1]])
```
